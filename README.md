# deep-learning-challenge

This deep learning challenge is for a non-profit foundation, Alphabet Soup, to determine to which applicants they should provide the funding. The CSV file contains historical data of organizations which have received fundings from Alphabet Soup, including name of the organizations, application type, affiliated sector of industry, government organization classification, use case for funding, organization type, active status, income classification, any special aspects to consider for application, funding amount, and finally whether the applicant was successful or not. With machine learning and neural networks, we set up a model and made it to predict for future applicants.
First model, saved under AlphabetSoupCharity.ipynb file, shows a binary classification model to predict which organization will be successful. In order to do this, we first dropped ‘EIN’ and ‘NAME’ columns. Next, we have created binning for application type and classification to prepare the data and make the “rare” categorical variables into a new value “other.” Then we separated the target array, y, which was the column ‘IS_SUCCESSFUL’ in this case, and the features array, X. After we have trained and tested the features, we compiled and trained the model. In order to do this, we had two hidden layers, with 80 and 30 neurons, respectively, and used the “relu” function. Then we had the “sigmoid” function with 1 node as the outer layer to create binary classifier model. As a result, the accuracy of this model came out to be 72.5%. 
In order to increase the accuracy to be over 75%, there were several trials and errors I had to go through. First, I tried increasing the epochs from 100 to 200, but the accuracy did not change much and remained at 72.58%. Then, I tried to put a third layer with “sigmoid” function, right before the output layer to see if it improves the accuracy but it did not, and still remained at 72.5%. 
Next, I went back to the top of the jupyter notebook, and only dropped the ‘EIN’ column and left ‘NAME’ column active to create more binning. Initially, I had set 400 as the number of values for NAME binning, but the accuracy remained same at 72.5% (the work saved under AlphabetSoupCharity_Optimization.ipynb).
Finally, I changed the number of values for NAME binning to 100, changed the hidden nodes to 10 and 5, respectively, the accuracy rose upto 75.1% (saved under AlphabetSoupCharity_Optimization_2.ipynb).

The final report on the neural network model is saved as Deep learning challenge analysis.pdf.
